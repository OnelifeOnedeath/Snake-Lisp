#!/usr/bin/env python3
"""
Snake-Lisp Lexer üêç
–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ –≤ –ø–æ—Ç–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
"""

import re

class Token:
    """–¢–æ–∫–µ–Ω Lisp –∫–æ–¥–∞"""
    def __init__(self, type, value, line, column):
        self.type = type
        self.value = value
        self.line = line
        self.column = column
    
    def __repr__(self):
        return f"Token({self.type}, {repr(self.value)}, {self.line}:{self.column})"

class Lexer:
    """–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä Snake-Lisp"""
    
    # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤
    TOKEN_SPEC = [
        ('COMMENT', r';[^\n]*'),           # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        ('LPAREN', r'\('),                 # –û—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
        ('RPAREN', r'\)'),                 # –ó–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞  
        ('NUMBER', r'-?\d+\.?\d*'),        # –ß–∏—Å–ª–∞
        ('STRING', r'"[^"]*"'),            # –°—Ç—Ä–æ–∫–∏
        ('SYMBOL', r'[^\s()";]+'),         # –°–∏–º–≤–æ–ª—ã
        ('WHITESPACE', r'\s+'),            # –ü—Ä–æ–±–µ–ª—ã (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
    ]
    
    def __init__(self, code):
        self.code = code
        self.tokens = []
        self.line = 1
        self.column = 1
        self.pos = 0
        self.compile_regex()
    
    def compile_regex(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–∫–∏ –≤ –æ–¥–∏–Ω –ø–∞—Ç—Ç–µ—Ä–Ω"""
        patterns = []
        for name, pattern in self.TOKEN_SPEC:
            patterns.append(f'(?P<{name}>{pattern})')
        self.pattern = re.compile('|'.join(patterns))
    
    def tokenize(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        while self.pos < len(self.code):
            match = self.pattern.match(self.code, self.pos)
            if not match:
                raise SyntaxError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª: {self.code[self.pos]} at {self.line}:{self.column}")
            
            kind = match.lastgroup
            value = match.group()
            
            if kind == 'WHITESPACE':
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é
                lines = value.count('\n')
                if lines > 0:
                    self.line += lines
                    self.column = len(value) - value.rfind('\n')
                else:
                    self.column += len(value)
            elif kind == 'COMMENT':
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                pass  
            else:
                # –°–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω
                token = Token(kind, value, self.line, self.column)
                self.tokens.append(token)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é
                self.column += len(value)
            
            self.pos = match.end()
        
        return self.tokens

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞—à –ª–µ–∫—Å–µ—Ä
if __name__ == "__main__":
    code = """
    ; –≠—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    (+ 1 2 3)
    (define pi 3.14159)
    "hello world"
    """
    
    lexer = Lexer(code)
    tokens = lexer.tokenize()
    
    for token in tokens:
        print(token)
